# fm-playlist-recommender


---

Recommender system for generating music playlists for a fictional _"Pozytywka"_ streaming service. Based on [LightFM](https://making.lyst.com/lightfm/docs/lightfm.html) algorithm. The system is capable of generating playlists which should match the music taste of multiple users at once. A microservice for serving recommendations is also included.

The model is trained on a dataset containing user-track interactions plus user / track metadata. Single-user recommendations are generated by factorizing the user-track interaction matrix. To solve the cold-start problem, tracks are represented by $k$-dimensional latent factors, which are derived from their metadata. More on that in the [LightFM paper](https://arxiv.org/pdf/1507.08439.pdf). The model generates large amounts of recommendations for each user and picks $N$ tracks with best average rank.

Check out the [example notebook](src/example.ipynb).


## Project structure
- `data` - subsequent data versions for training the model and building the service (removed, since we received this data from the university and it is not available publicly)
- `doc` - project documentation
- `src` - source code + example notebook

---

### Setup 

Java is required to query data with `pyspark` - JDK 17 is recommended. On top of that, install Python dependencies:

```shell
cd src
poetry shell
pip install lightfm==1.17 --no-use-pep517
poetry install
```

Note - you will need to manually copy data to `data/v4`.

### Train the model
```shell
cd src
python3 create_models.py
```

This will train the model and serialize both base and _advanced_ model.
Adjust params in code. 
Serialized models will be written to `src/data/serialized/`

### Run the service

Run with docker:
```shell
docker build -t ium .
docker run -p 8081:8081 -v ${PWD}:/predictions ium
```

All recommendations along with provided input and saved metadata will be saved to `/predictions/predictions.jsonl`.

**WARNING**: Bear in mind that build context and unused volume may (will) occupy large amounts of space, as docker will need to transfer the entire `data` directory to build daemon. A `docker system prune` may come in handy if you run out of space.

### Make recommendations

Following endpoints are available:
- `/predict` - make recommendations either with base or advanced model (A/B test). Exact model is chosen based on supplied user ID.
- `/predict/base` - predict using base model - sample songs from users' session history
- `/predict/advanced` - predict using the FM model

Example request & response:

```shell
curl -X 'POST' \
    'http://localhost:8081/predict' \
    -H 'accept: application/json' \
    -H 'Content-Type: application/json' \
    -d '{
    "user_id": 123,
    "other_users": [
      124, 125, 126, 127, 128
    ],
    "playlist_length": 20
}'
```

```json lines
[
    {
        "id": "2iUmqdfGZcHIhS3b9E9EWq",
        "name": "Everybody Talks",
        "artist": "Neon Trees"
    },
    {
        "id": "5FPnjikbwlDMULCCCa6ZCJ",
        "name": "Daughters",
        "artist": "John Mayer"
    },
    {
        "id": "3bXhtg6H8lOMWaLZttQF6F",
        "name": "Sunday Morning - Acoustic",
        "artist": "Maroon 5"
    },
    {
        "id": "0HRshWRNAwQBROvxXqG3i9",
        "name": "Skinny Love",
        "artist": "Birdy"
    },
    { "and so on" }
]
```


For exact request and response schemas check Swagger documentation at `<HOST>/doc`.

